{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Project: Sentiment Classification (imdb reviews)\n",
        "\n",
        "# By: Krishna Kant Kaushal\n",
        "\n",
        "##### Python version used: Python 3.7.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3JSa470htrR"
      },
      "source": [
        "### ü•èProblem Description:\n",
        "Generate Word Embedding and retrieve outputs of each layer with Keras based on the Classification task. Word embedding are a type of word representation that allows words with similar meaning to have a similar representation.\n",
        "\n",
        "It is a distributed representation for the text that is perhaps one of the key\n",
        "breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
        "\n",
        "We will use the IMDb dataset to learn word embedding as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with a sentiment (positive or negative)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC4PaxJDhtrS"
      },
      "source": [
        "### ü•èData Description:\n",
        "The Dataset of 25,000 movie reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000.\n",
        "\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to\n",
        "encode any unknown word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "## ü•èImport the required librarries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5dp0L-kqhtrT",
        "outputId": "2a33a124-c495-4ebc-e10f-84f60ca020b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "# for array manupulation\n",
        "import numpy as np\n",
        "\n",
        "# for keras models\n",
        "from keras import models\n",
        "\n",
        "# for individual layers in the sequential model\n",
        "from keras import layers\n",
        "\n",
        "# for loading imdb data\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# to make all sequences in a list have same length\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# for retrieving the output of each layers\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzJsMxxzhtrU"
      },
      "source": [
        "## ü•è1. Import test and train data & \n",
        "## ü•è2. Import the labels (train and test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGCtiXUhSWss",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000 #vocab size\n",
        "#number of word used from each review\n",
        "maxlen = 20  # Using last 20 words from each review to speed up training.\n",
        "\n",
        "# load dataset as a list of ints\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GupMceuvhtrV"
      },
      "outputs": [],
      "source": [
        "# make all sequences of the same length\n",
        "training_data = pad_sequences(training_data, maxlen=maxlen)\n",
        "testing_data =  pad_sequences(testing_data, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss-aqpCYhtrV"
      },
      "outputs": [],
      "source": [
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEpRPELFhtrV"
      },
      "source": [
        "#### Exploring the Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "l06c6N_zhtrV",
        "outputId": "40874c7c-7e51-4395-a7e8-984435174044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categories: [0 1]\n",
            "Number of unique words: 9858\n",
            "Average Review length: 20.0\n",
            "Standard Deviation: 0.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNwY-nnchtrW"
      },
      "source": [
        "Single training example - first review of the dataset, which is labeled as positive (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JdFR-J_BhtrW",
        "outputId": "54d7a3f5-30cd-481a-f8db-d7bccd7abd1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: 1\n",
            "\n",
            " [  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
            "   15   16 5345   19  178   32]\n"
          ]
        }
      ],
      "source": [
        "print(\"Label:\", targets[0])\n",
        "print(\"\\n\",data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoknz0j8htrW"
      },
      "source": [
        "The code below retrieves the dictionary mapping word indices back into the original words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wb0v9-ahhtrW",
        "outputId": "1e002f9b-8db2-4dc2-9b64-dbb8ea8b82b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
          ]
        }
      ],
      "source": [
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i, \"#\") for i in data[0]] )\n",
        "print(decoded) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JJJPIaihtrW"
      },
      "source": [
        "Data preparation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JlzFNtqghtrX"
      },
      "outputs": [],
      "source": [
        "def vectorize(sequences, dimension = 10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1\n",
        "    return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7dSXqaYhtrX"
      },
      "source": [
        "Split our data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "uf1pjYpPhtrX"
      },
      "outputs": [],
      "source": [
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "48FBB0V5htrX",
        "outputId": "f3d59020-5e76-488c-b8ca-370f208549d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 1., 0., 0.]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "34oQxKqEhtrX",
        "outputId": "e23086b3-75fe-4e39-8e90-b3ff1e219d8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 10000) (40000, 10000)\n",
            "(10000,) (40000,)\n"
          ]
        }
      ],
      "source": [
        "print(test_x.shape, train_x.shape)\n",
        "print(test_y.shape, train_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4WGuLI1htrX"
      },
      "source": [
        "## ü•è3. Get the word index and create a key-value pair for word and word_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "iLKEwUAShtrX"
      },
      "outputs": [],
      "source": [
        "# Get the word index\n",
        "index = imdb.get_word_index()\n",
        "# To print word index, remove comment (#) from below line\n",
        "# print(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "2n4rzMGIhtrY"
      },
      "outputs": [],
      "source": [
        "# create a key-value pair for word and word_id\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
        "\n",
        "# To print 'key-value' pair for 'word' and 'word_id', remove comment (#) from below line\n",
        "# print(reverse_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHKKDN1-htrY"
      },
      "source": [
        "## ü•è4. Build a Sequential Model using Keras for the Sentiment Classification task "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgOHs3D3htrY"
      },
      "source": [
        "#### Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LDGAWD49htrY"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YaUaPeyKhtrY"
      },
      "outputs": [],
      "source": [
        "# Input - Layer\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "\n",
        "# Hidden - Layers\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "          \n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "\n",
        "# Output- Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "OfEH73CzhtrY",
        "outputId": "0cf39ad0-71b7-4652-fe54-4719c2647ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 505,201\n",
            "Trainable params: 505,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yV5Ykfo5htrY"
      },
      "outputs": [],
      "source": [
        "# Compiling the model\n",
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "cVXoV7VNhtrZ",
        "outputId": "26da6d50-40b8-4e2a-fbec-6b11256348ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 7s 187us/step - loss: 0.5951 - accuracy: 0.6719 - val_loss: 0.4851 - val_accuracy: 0.7647\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 6s 159us/step - loss: 0.4324 - accuracy: 0.8016 - val_loss: 0.4707 - val_accuracy: 0.7639\n"
          ]
        }
      ],
      "source": [
        "# Fitting the model\n",
        "results = model.fit(\n",
        " train_x, train_y,\n",
        " epochs= 2,\n",
        " batch_size = 500,\n",
        " validation_data = (test_x, test_y)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igq8Qm8GeCzG"
      },
      "source": [
        "## ü•è5. Report the Accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "maRwz3VGhtrZ",
        "outputId": "78b7f656-4345-4a9e-aa9f-c1633a728cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model is : 73.67249727249146 %\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of the model is :\", 100*np.mean(results.history[\"accuracy\"]),\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glldaBiFhtrZ"
      },
      "source": [
        "<b>Note</b>: \n",
        "> 1. The test-accuracy is a bit low as only 20 words of reviews is used. \n",
        "> 2. Incresing the words in review may well result in increase in accuracy. \n",
        "> 3. Increasing epoch no. may also result in increase in accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DBBpbulhtrZ"
      },
      "source": [
        "#### Lets check the accuracy with 200 words review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUcTuaAqhtrZ",
        "outputId": "10ef953f-2788-4cf5-83fe-9f0faab87a4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 8s 208us/step - loss: 0.2923 - accuracy: 0.8805 - val_loss: 0.2773 - val_accuracy: 0.8873\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 8s 201us/step - loss: 0.1828 - accuracy: 0.9302 - val_loss: 0.2971 - val_accuracy: 0.8824\n",
            "\n",
            "\n",
            "Accuracy of the model is : 90.53249955177307 %\n"
          ]
        }
      ],
      "source": [
        "maxlen = 200  # Using the last 200 words from each review\n",
        "\n",
        "# load dataset as a list of ints\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# make all sequences of the same length\n",
        "training_data = pad_sequences(training_data, maxlen=maxlen)\n",
        "testing_data =  pad_sequences(testing_data, maxlen=maxlen)\n",
        "\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
        "\n",
        "# Vectorize every review and filling it with zeros so it contains exactly 10,000 numbers.\n",
        "def vectorize(sequences, dimension = 10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1\n",
        "    return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")\n",
        "\n",
        "# Splitting the dataset in test and train sets\n",
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]\n",
        "\n",
        "# Fitting the model\n",
        "results = model.fit(\n",
        " train_x, train_y,\n",
        " epochs= 2,\n",
        " batch_size = 500,\n",
        " validation_data = (test_x, test_y)\n",
        ")\n",
        "\n",
        "# Accuracy of the model\n",
        "print(\"\\n\\nAccuracy of the model is :\", 100*np.mean(results.history[\"accuracy\"]),\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk6u2uVehtra"
      },
      "source": [
        "Note:\n",
        "> We see here that there is improvement in model accuracy after increasing the no. of words in review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnbPiWIEhtra"
      },
      "source": [
        "#### Now lets check the accuracy with epoch = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn4QIg69htra",
        "outputId": "c3f93dd1-04fa-4d93-9a9a-b3e76fe6187e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 6s 149us/step - loss: 0.1155 - accuracy: 0.9598 - val_loss: 0.3774 - val_accuracy: 0.8780\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 5s 128us/step - loss: 0.0726 - accuracy: 0.9763 - val_loss: 0.4196 - val_accuracy: 0.8721\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 5s 122us/step - loss: 0.0511 - accuracy: 0.9823 - val_loss: 0.5085 - val_accuracy: 0.8724\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 5s 135us/step - loss: 0.0402 - accuracy: 0.9867 - val_loss: 0.5264 - val_accuracy: 0.8702\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 5s 130us/step - loss: 0.0362 - accuracy: 0.9872 - val_loss: 0.5791 - val_accuracy: 0.8723\n",
            "\n",
            "\n",
            "Accuracy of the model is : 97.84649610519409 %\n"
          ]
        }
      ],
      "source": [
        "# Fitting the model with epoch = 5\n",
        "results = model.fit(\n",
        " train_x, train_y,\n",
        " epochs= 5,\n",
        " batch_size = 500,\n",
        " validation_data = (test_x, test_y)\n",
        ")\n",
        "\n",
        "# Accuracy of the model\n",
        "print(\"\\n\\nAccuracy of the model is :\", 100*np.mean(results.history[\"accuracy\"]),\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kS1SRdghtra"
      },
      "source": [
        "Note:\n",
        "> We see here that there is further improvement in model accuracy after increasing the epoch value from 2 to 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSHZLLSXhtra"
      },
      "source": [
        "## ü•è6. Retrive the output of each layer in keras for a given single test sample from the trained model you built"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOS6cYQshtra"
      },
      "source": [
        "Retrive output of each layer for first test sample i.e. test_x[:1,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "SIVRLzWohtra",
        "outputId": "a36ea832-9ba8-428a-cbdc-416f64048e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output of layer 1 is:\n",
            "\n",
            " [array([[0.6408298 , 0.53438866, 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.214016  , 0.        ,\n",
            "        0.48909628, 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.20184799, 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.00532168, 0.        , 0.        , 0.07950675,\n",
            "        0.        , 0.07530031, 0.        , 0.84624875, 0.        ,\n",
            "        0.        , 0.6949614 , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.22178644,\n",
            "        0.        , 0.        , 0.        , 0.70583624, 0.27183208,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32)]\n",
            "\n",
            "Output of layer 2 is:\n",
            "\n",
            " [array([[0.6408298 , 0.53438866, 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.214016  , 0.        ,\n",
            "        0.48909628, 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.20184799, 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.00532168, 0.        , 0.        , 0.07950675,\n",
            "        0.        , 0.07530031, 0.        , 0.84624875, 0.        ,\n",
            "        0.        , 0.6949614 , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.22178644,\n",
            "        0.        , 0.        , 0.        , 0.70583624, 0.27183208,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
            "      dtype=float32)]\n",
            "\n",
            "Output of layer 3 is:\n",
            "\n",
            " [array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.72456646, 0.        , 1.0432608 , 0.        ,\n",
            "        0.        , 0.9745749 , 0.75828224, 0.66064435, 0.8556126 ,\n",
            "        0.        , 0.02812914, 0.        , 1.1255662 , 0.        ,\n",
            "        0.6485625 , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 1.0842075 , 0.67425454, 0.        , 0.        ,\n",
            "        0.        , 0.74884176, 0.48468512, 0.        , 0.71991295,\n",
            "        0.        , 0.        , 0.67103136, 0.7888963 , 0.44960183,\n",
            "        0.        , 0.8672092 , 0.        , 0.38242167, 0.        ,\n",
            "        0.        , 0.        , 1.1843905 , 0.9268881 , 0.        ]],\n",
            "      dtype=float32)]\n",
            "\n",
            "Output of layer 4 is:\n",
            "\n",
            " [array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.72456646, 0.        , 1.0432608 , 0.        ,\n",
            "        0.        , 0.9745749 , 0.75828224, 0.66064435, 0.8556126 ,\n",
            "        0.        , 0.02812914, 0.        , 1.1255662 , 0.        ,\n",
            "        0.6485625 , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.        , 1.0842075 , 0.67425454, 0.        , 0.        ,\n",
            "        0.        , 0.74884176, 0.48468512, 0.        , 0.71991295,\n",
            "        0.        , 0.        , 0.67103136, 0.7888963 , 0.44960183,\n",
            "        0.        , 0.8672092 , 0.        , 0.38242167, 0.        ,\n",
            "        0.        , 0.        , 1.1843905 , 0.9268881 , 0.        ]],\n",
            "      dtype=float32)]\n",
            "\n",
            "Output of layer 5 is:\n",
            "\n",
            " [array([[0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
            "        0.       , 2.6080246, 0.       , 0.       , 2.919148 , 0.       ,\n",
            "        0.       , 3.0893252, 0.       , 2.7356572, 0.       , 2.9284673,\n",
            "        0.       , 0.       , 3.8103337, 0.       , 0.       , 1.9385139,\n",
            "        0.       , 0.       , 3.259725 , 0.       , 0.       , 3.0802207,\n",
            "        0.       , 0.       , 3.3439906, 0.       , 2.9719126, 0.       ,\n",
            "        0.788835 , 2.9316728, 0.       , 3.2971592, 3.2102857, 0.       ,\n",
            "        0.       , 3.3036313, 2.755447 , 0.       , 0.       , 0.       ,\n",
            "        3.1827095, 2.5342667]], dtype=float32)]\n",
            "\n",
            "Output of layer 6 is:\n",
            "\n",
            " [array([[0.9999999]], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "i=1\n",
        "for layer in model.layers:\n",
        "    keras_function = K.function([model.input], [layer.output])\n",
        "    print(\"\\nOutput of layer\", i,\"is:\\n\\n\", keras_function([test_x[:1,:], 1]))\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gfv9Szzhtrb"
      },
      "source": [
        "> <b>Note</b>: The value of output layer for input <b>test_x[:1,:]</b> should be equal to that of actual output i.e. <b>test_y[:1]</b>. \n",
        "\n",
        "Lets check that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AqOnLa2eCzH",
        "scrolled": true,
        "outputId": "ee4e567c-88ab-4616-a19e-91419e8ae37c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.], dtype=float32)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_y[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPvsRLOZhtrb"
      },
      "source": [
        "> So for input <b>test_x[:1,:]</b>, the actual output = predicted output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP1590ZJhtrb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}